---
title: "SBD2_loan_sample_09"
author: "Ambrosioni Hélène, Sivakolunthu Nerome, Kuster Dario, Pagliarin Larissa"
date: '2023'
output: html_document
df_print: paged
---

# Inroduction

>xy



***



## Prepare workplace

```{r include=FALSE, echo=FALSE}
rm(list=ls())

set.seed(7)
```

### Install libraries

```{r}
knitr::opts_chunk$set(echo = TRUE)

libraries = c("readr", "ggplot2", "dlookr", "dplyr", "RColorBrewer", "DescTools", "ROSE", "ggcorrplot", "car", "plotly", "tidyverse", "corrplot", "GGally", "gridExtra","caret", "ROCR")

lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})

lapply(libraries, library, quietly = TRUE, character.only = TRUE)
```



### Read data

> We import the csv file "loan_sample_9.csv" and make a copy of it to ensure that we don't mess up the original dataset.

```{r}
data_loans <- read_csv("loan_sample_9.csv")
data <- data_loans
```
***

## Descriptive analysis 

### Check the data

> In the first step we explore the data. We start by investigating the structure of the data set.
There are 12 numeric and 5 categorical variables in the dataset. But the numeric variable "Status" with its values "1" and "0" looks like a factor and all the characteristic variables also look like factors.

```{r, echo=FALSE}
head(data)
tail(data)
str(data)
```
***

### Data quality issues - Checking for NAs

> We check the presence of NAs in each of the variables included in the dataset.
There are no NAs values in this dataset.

```{r}
knitr::kable(apply(data, 2, function(x) any(is.na(x))))
```
***

### What data types are included in the data set?

> Now we have 12 numeric and 5 character variables.

```{r}
overview <- overview(data)
plot(overview)
```
***

### Transform some variables

> We transform the characteristic variables in factors to count the categories and order them.

```{r}
data$grade = as.factor(data$grade)
data$home_ownership = as.factor(data$home_ownership)
data$verification_status = as.factor(data$verification_status)
data$purpose = as.factor(data$purpose)
data$application_type = as.factor(data$application_type)
data$Status = as.factor(data$Status)

data <- data %>%
  select(order(sapply(., is.factor)),order(sapply(., is.numeric)))
```

```{r}
overview <- overview(data)
plot(overview)
```
***

### Summary of variables

#### Nummeric Variables

> In most numerical variables there is a large gap between the minimum and maximum.
For example, "loan-amnt" (amount of the loan applied for by the borrower) has a minimum of 1,000 and a maximum of 40,000, or "revol_bal" (Total credit revolving balance) from USD 0 to USD 78,762.
> The average interest rate "int_rate" is around 12.63%, with values between 5.31% and 27.49%.
> The annual income "annual_inc" of borrowers varies greatly, with an average of around USD 63,277.
There are outliers with very high annual salaries.
> There are borrowers with a dti of 0, which could indicate low indebtedness.


#### Variable "purpose"

>The Variable "purpose" (category provided by the borrower for the loan request) has many categories. They contain the name of the type of loan, except for one group. This group is labeled as "other" and contains 2,283 values. Most loans are used for debt consolidation and credit cards.


#### Variable "grade"

> The most people are graded between "B" and "C", in the grades "A" or "B" are similar number of people. The variable "grade" assigned loan grade by the financial service provider.


#### Variable "home_ownership"

> The most people are in rent or has a mortgage for there home. 3,982 people are home owner.
14,278 people from 40,000 aren't verified.


#### Variable "verification_status"

> We see that 14,278 people are not verifide from 40,000 people. 16,129 are source verifide.


#### Variable "application_type"

> Only 530 joined via App from 40,000 people in the System.


#### Variable "Status"

> The target variable "Status" is unbalanced, as there are more loans without default (status 0 = 34,794 persons) than with default (status 1 = 5,206).

```{r}
summary(data)
```
***

### Balance of the target variable  

> In the next step, we investigate our target variable "Status". We notice also before in our sample, that we have 5,206 persons which did not default on their loan and we have 34,794 which did default. 

> As we can see in the visualization the data set is highly imbalanced.

```{r}
ggplot(data, aes(x = Status, fill = Status)) +
  geom_bar() +
  ylab("Count") +
  xlab("Status of the loan")
```

```{r}
PercTable(data$Status)
```
***

> In the next step, we carry-out under sampling and visualizate it again. 

```{r}
set.seed(7)
data_original <- data
data_balanced <- ovun.sample(Status ~ ., data=data, method = "under")
data_under <- data.frame(data_balanced[["data"]])
```

```{r, echo=FALSE}
ggplot(data_under, aes(x = Status, fill = Status)) +
  geom_bar() +
  ylab("Count") +
  xlab("Status of the loan")
```


***

> Visualization of the level of the target variable

```{r, echo=FALSE}
corr_data <- cor(data[1:11])
p_value_data <- cor_pmat(data[1:11])
ggcorrplot(corr_data, type = "lower",
           p.mat = p_value_data,
           outline.col = "white",
           ggtheme = ggplot2::theme_gray,
           colors = c("tomato2", "white", "skyblue2"),
           lab = TRUE,
           title = "Correlation matrix of the unbalanced variables")
```

```{r, echo=FALSE}
corr_data_under <- cor(data_under[1:11])
p_value_data_under <- cor_pmat(data[1:11])
ggcorrplot(corr_data_under, type = "lower",
           p.mat = p_value_data_under,
           outline.col = "white",
           ggtheme = ggplot2::theme_gray,
           colors = c("tomato2", "white", "skyblue2"),
           lab = TRUE, 
           title = "Correlation matrix of the balanced variables")
```

***

> Balancing the data set is necessary to ensure that the model is not influenced by an excessive presence of one class over the other. Models can tend to focus on the dominant class and ignore the minority class if the data set is not balanced. Balancing ensures that both classes are equally represented, which can lead to a more balanced model.



### Distribution of the numeric variables


```{r fig.width=15, fig.height=5, echo=FALSE}

create_numeric_plots <- function(data, variable) {
  
  # Histogramm
  hist_plot <- ggplot(data, aes(x = !!sym(variable))) +
    geom_histogram(fill = "#00CED1", color = "white", bins = 30) +
    labs(title = paste("Distribution of", variable), x = variable, y = "Frequency") +
    theme_minimal()

  # Boxplot
  box_plot <- ggplot(data, aes(y = !!sym(variable))) +
    geom_boxplot(fill = "#00CED1", color = "black") +
    labs(title = paste("Boxplot of", variable), y = variable) +
    theme_minimal()

  # Kernel Density Plot
  density_plot <- ggplot(data, aes(x = !!sym(variable))) +
    geom_density(fill = "#00CED1", color = "white") +
    labs(title = paste("Kernel Density Plot of", variable), x = variable, y = "Density") +
    theme_minimal()

  # List of Plots
  return(list(hist_plot, box_plot, density_plot))
}


numeric_variables <- c("loan_amnt", "int_rate", "annual_inc", "dti", "open_acc", "revol_bal",
                        "revol_util", "total_acc", "total_rec_int", "tot_cur_bal", "total_rev_hi_lim")


for (variable in numeric_variables) {
  plots <- create_numeric_plots(data, variable)
  grid.arrange(grobs = plots, ncol = 3)
}

```



***


### Cheking for outliers 

> We provide a boxplot of the numeric variables in both the original and under-sampled dataset. 
 
```{r, fig.width=20, fig.height=20, echo=FALSE}
# Simple visualization of the full data 
boxplot(scale(data[,1:11]), use.cols = TRUE)
```



***


```{r}
knitr::kable(diagnose_outlier(data_under), caption = "Diagnose Outlier", digits = 2)
```
***


#### Visualzation with and without the outliers. 

> We note that for the variables "annual_inc" (The self-reported annual income provided by the borrower during registration) the visualization changes considerably and there the median also tends to shift strongly.

```{r, echo=FALSE}
data_under %>%
  plot_outlier(diagnose_outlier(data_under) %>%
                 filter(outliers_ratio >= 0.5) %>%          # dplyr
                 select(variables) %>%
                 unlist())
```



***



#### Dealing with outliers 

> We do Interquartil Range (IQR) methode for dealing with the highest outliers.

```{r}
outlier <- function(x){
    quantiles <- quantile(x, c(.05, .95))
    x[x < quantiles[1]] <- quantiles[1]
    x[x > quantiles[2]] <- quantiles[2]
    x
}

data_new_under <- map_df(data_under[,-c(12:17)], outlier)
cols <- data_under[,c(12:17)]
data_new_under <- cbind(data_new_under, cols)
```

```{r, fig.width=20, fig.height=20}
boxplot(scale(data_new_under[,c(1:11)]), use.cols = TRUE)
```
***


```{r, echo=FALSE}
for (i in 1:length(data_new_under[,-c(12:17)])) {
  print(ggplot(data_new_under, aes(y = data_new_under[,i], color = Status)) + 
          geom_boxplot() + 
          ylab(names(data_new_under[i])) + 
          theme(axis.title.x=element_blank(),
                axis.text.x=element_blank(),
                axis.ticks.x=element_blank()))
}
```


***


```{r, echo=FALSE}
for (i in 1:length(data_new_under[,-c(12:17)])) {
  print(ggplot(data_new_under, 
               aes(x = data_new_under[,i],
                   fill = Status)) +
          geom_density(alpha = 0.2) +
          xlab(names(data_new_under[i])) +
          ylab("Density"))
}

```



***


```{r, echo=FALSE, fig.width=8, fig.height=5}
corr_data_new_under <- cor(data_new_under[,-c(12:17)])
p_value_data_new_under <- cor_pmat(data_new_under[,-c(12:17)])
p1 <- ggcorrplot(corr_data_new_under, type = "lower",
           p.mat = p_value_data_new_under,
           outline.col = "white",
           ggtheme = ggplot2::theme_gray,
           colors = c("tomato2", "white", "skyblue2"),
           lab = TRUE)

ggplotly(p1, tooltip = c("x", "y"))
```



***


```{r, echo=FALSE}
correlations = cor(data_new_under[-c(12:17)])
corrplot(correlations) 
```



***



```{r, fig.width=20, fig.height=20}
ggpairs(data[, c("loan_amnt", "int_rate", "annual_inc", "dti", "total_acc", "total_rec_int", "tot_cur_bal")], 
        aes(color = as.factor(data$Status)))
```



***


### Association between the loan amount requested and the annual income of the borrower

```{r, echo=FALSE}
plot_ly(data, x = ~loan_amnt, y = ~annual_inc, mode = "markers",
        type = "scatter", marker = list(color = "#00CED1", textfont = list(size = 8))) %>%
  layout(title = "Scatter Plot of Loan Amount vs. Annual Income",
         xaxis = list(title = "Loan Amount"),
         yaxis = list(title = "Annual Income"))
```


***



## Exercise 2

### Training and testing a logistic classifier
```{r}
set.seed(7)
div <- createDataPartition(y = data_new_under$Status, p = 0.7, list = F)

# Training Sample
data.train <- data_new_under[div,] # 70% here

# Test Sample
data.test <- data_new_under[-div,] # rest of the 30% data goes here
```


***


### Training the classifier

```{r}
fit1 <- glm(Status ~ ., data=data.train,family=binomial())
summary(fit1)
```

#### Explanation:

> From the results obtained, we can deduce the following points.
Deviance residuals:
* The moderate range of deviance residuals (-2.08 to 2.10) suggests a reasonable fit to the model. Smaller residuals would indicate a more precise fit, but these values are acceptable.
Coefficients:
* Positive coefficients for "loan_amnt" and "int_rate" indicate that higher loan amounts and interest rates are associated with a higher probability of belonging to the positive class.
Significance:
* Statistically significant predictors include "loan_amnt," "int_rate," "annual_inc," "total_rec_int," "gradeB," and "gradeC," crucial characteristics that influence the model.
Zero and residual deviance:
* The reduction of deviance from null model 10103.3 to residual deviance 9254.7 suggests that the model, explains some of the variability of the response variable.
AIC:
* The AIC value of 9318.7 is an indicator of model fit and complexity. Although it could be lower, the AIC is still a reasonable value considering the number of predictors.
<br>
Conduct cross-validation to ensure generalizability of the model.
In summary, the model shows promise with significant predictors, but there is room for improvement. Further analysis and refinement can improve its predictive capabilities and overall performance.


***


### Plotting the ROC Curve

```{r}
data.test$fit1_score <- predict(fit1,type='response',data.test)
fit1_pred <- prediction(data.test$fit1_score, data.test$Status)
fit1_roc <- performance(fit1_pred, "tpr", "fpr")
plot(fit1_roc, lwd=1, colorize = TRUE, main = "Fit1: Logit - ROC Curve")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1, lty=3)
```

#### Explanation

>Through the ROC (Receiver Operating Characteristic) curve, we can evaluate the performance of the classification algorithm.
There are several aspects that we can identify and comment on.
<br>
The shows the relationship between the true positive rate (sensitivity) and the false positive rate (1 - specificity) for different thresholds. The true positive rate is shown on the Y-axis and the false positive rate on the X-axis.
The diagonal line represents a random rate classifier. A good classifier is above this line, which means it achieves a higher true positive rate than false positive rate for different thresholds.
On the right we have the color scale which represents the threshold at which the corresponding rate is reached. The red areas represent higher thresholds and the blue areas lower thresholds.
<br>
In our case the curve appears to be well above the diagonal, indicating a better classifier than a random guess. The color scale can be useful to see how thresholds affect evaluation metrics.


***



### visualizing the Precision/Recall Curve

```{r}
fit1_precision <- performance(fit1_pred, measure = "prec", x.measure = "rec")
plot(fit1_precision, main="Fit1: Logit - Precision vs Recall")
```

#### Explanation:

> With the precision recall curve we can evaluate the classification model used and understand whether the classes are unequally distributed or not.
In the graph, the x and y axes are called recall and precision respectively:
Recall (X-axis): Percentage of actual positive cases that were recognized as positive. Recall is a measure of how many of the actual positive cases the model correctly identified.
Precision (Y-axis): Proportion of relevant instances among instances classified as positive. Precision is a measure of how many of the cases classified as positive are actually positive.
The curve in the graph shows the trade-off between precision and recall for different thresholds. Perfect classification would produce a curve at the top right of the graph where both precision and recall are 1.
In this case, precision starts to be high when recall is low. This means that the model is very selective when it decides to classify an instance as positive. As recall increases (the model tries to capture more true positive cases), precision decreases. This is a typical trade-off, as it is often difficult to achieve high precision and high recall at the same time.
Since the curve is directed upwards in the right corner, it can be deduced that there is high precision and recall.


***



### Confusion Matrix

```{r}
confusionMatrix(as.factor(round(data.test$fit1_score)), data.test$Status)
```

#### Explanation:

Confusion Matrix
* It correctly identifies 64.19% of instances (Accuracy) with 62.65% sensitivity (True Positive Rate) and 65.73% specificity (True Negative Rate).
Kappa Statistic
* The Kappa value of 0.2838 indicates fair agreement beyond random chance.
Positive Predictive Value (Precision):
* Precision is at 64.64%, meaning that when the model predicts the positive class, it is correct 64.64% of the time.
Balanced Accuracy:
* The balanced accuracy is 64.19%, reflecting a balance between sensitivity and specificity.
Prevalence and Detection Rate:
* The prevalence of the positive class is 50%, and the model detects it in 31.33% of cases.
Mcnemar's Test
* McNemar's test does not show a significant difference in errors between predictions.
In conclusion, the model demonstrates moderate performance, but there is room for improvement.


***

### Computing the predictive utility of the model through the area under the curve AUC value

```{r}
fit1_auc <- performance(fit1_pred, measure = "auc")
cat("AUC: ",fit1_auc@y.values[[1]]*100)
```


#### Explanation:

>The AUC-Value of 70.46687 falls in to the fair discrimination range. While it suggests some ability of our model to distinguish between the two classes, there is definelty room for improvement. It could be valuable to compare our AUC-Value to that of other models to gain further context regarding our model's performance.

## Exercise 3

### Improve pre-processing of data

#### Variables

> Loan utilization ratio:
<br>
This ratio could indicate how much of the available loan has already been used.

```{r}
summary(data$loan_amnt / data$total_rev_hi_lim)
```

> Income to loan ratio:
<br>
This could indicate how well the borrower is able to repay the loan, based on their income.

```{r}
summary(data$annual_inc / data$loan_amnt)
```

> Risikokategorien für den Schulden-zu-Einkommen (DTI):
<br>
A new categorical variable that categorizes debt-to-income (dti) into different risk categories. For example, low, medium and high risk based on certain thresholds.

```{r}
summary(cut(data$dti, breaks = c(-Inf, 15, 25, Inf), labels = c("Low Risk", "Medium Risk", "High Risk")))
```




## Exercise 4

### Question 1

What challenges in making credit decisions would a company face if it were to use our model in its day-to-day business?
These challenges are captured in the four common ethical issues in the context of creating value from data:

Privacy and data security
* Collecting and using various financial and personal variables (e.g., "loan_amnt", "int_rate", "annual_inc") for credit decisions requires a strong privacy framework for used customer data. Ensuring encryption, secure storage and compliance with privacy regulations are critical, considering the sensitive nature of financial information.

Algorithmic bias and fairness
* The model coefficients reveal that some variables, such as “grade B” and “grade C”, have a significant impact on the predictions. It is essential to carefully examine these variables for possible biases, ensuring that credit decisions are fair and impartial across different grades and demographic groups.

Accountability and Accountability
* Model performance parameters, including accuracy and sensitivity, provide a basis for evaluating its effectiveness. Establishing accountability for model results is critical, especially with significant predictors like “loan_amnt” and “int_rate.” Transparent communication about how decisions are made is essential for accountability.

Impact on the workforce
* Implementing the credit decision model may impact the workforce involved in manual credit assessments. Workforce implications, including potential job role changes, should be considered. Ethical considerations involve transparent communication about these changes and efforts to mitigate any negative impacts on the workforce.

In conclusion, while the logistic regression model shows promise in predicting credit decisions, addressing ethical issues requires a comprehensive approach. Ensure rigorous data privacy measures, continually evaluate and mitigate algorithmic bias, establish accountability for model results, and consider social impact on the workforce. Engaging in ongoing ethical discussions and staying attuned to the implications of model decisions will contribute to responsible and ethical implementation in daily business operations.


***


### Question 2

Companies can overcome or mitigate the problems and difficulties described above associated with implementing predictive models, particularly in credit decision making
In the following way: 

Data Privacy & Security: Implement Robust Security Measures
* Employ encryption and secure storage protocols to protect sensitive data.
Adopt anonymization and aggregation techniques to minimize the exposure of individual details.
Ensure compliance with data protection regulations and obtain explicit consent from individuals for data usage.

Algorithmic Bias & Fairness: Continuous Monitoring and Fairness Audits
* Regularly monitor and assess model predictions for biases.
Conduct fairness audits, particularly focusing on variables with significant impact.
Adjust the model as needed to ensure fairness across different demographic groups.

Accountability & Responsibility: Establish Clear Accountability and Transparency
*Clearly define roles and responsibilities for individuals involved in model development and deployment.
Maintain transparent documentation of the model's decision-making process.
Establish mechanisms for accountability and redress in case of errors or unintended consequences.

Impact on the Workforce: Responsible Workforce Management
* Provide training and upskilling opportunities for employees affected by automation.
Communicate transparently about changes in job roles or responsibilities.
Consider the societal impact and contribute to initiatives that support workforce development in the face of technological advancements.

By adopting these strategies, companies can navigate the ethical challenges associated with deploying predictive models for credit decisions, fostering responsible and transparent practices in their daily business operations. Regular reassessment and adaptation to evolving ethical standards and regulations are essential for continued ethical performance































